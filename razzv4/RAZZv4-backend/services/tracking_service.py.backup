""""""

People Detection and Tracking Service - EXACT COPY from BRINKSv2Advanced Multi-Object Tracking Service

Uses ByteTrack (30 FPS) + DeepSORT ReID (for uncertain tracks only)Implements ByteTrack and DeepSORT with conditional tracking strategies

"""Optimized for WebRTC streaming and real-time performance

"""

import cv2

import numpy as npimport logging

from collections import defaultdict, dequeimport numpy as np

from datetime import datetimefrom typing import List, Tuple, Dict, Optional

import timefrom dataclasses import dataclass, field

from typing import Dict, List, Tuple, Optionalfrom enum import Enum

import supervision as svimport cv2

from deep_sort_realtime.deepsort_tracker import DeepSortfrom filterpy.kalman import KalmanFilter

import loggingfrom scipy.optimize import linear_sum_assignment

from collections import deque, defaultdict

logger = logging.getLogger(__name__)import time



logger = logging.getLogger(__name__)

class TrackingService:

    """

    Advanced people detector with ByteTrack (30 FPS) + DeepSORT ReID fallbackclass TrackState(Enum):

    EXACT implementation from BRINKSv2    """Track lifecycle states"""

    """    NEW = 1

        TRACKED = 2

    def __init__(self, yolo_model, device: str = 'cuda', conf_threshold: float = 0.5,     LOST = 3

                 bytetrack_threshold: float = 0.6):    REMOVED = 4

        """

        Initialize the tracking service with ByteTrack and DeepSORT

        @dataclass

        Args:class Detection:

            yolo_model: Already loaded YOLO model    """Person detection with confidence and features"""

            device: Device to use ('cuda' or 'cpu')    bbox: np.ndarray  # [x1, y1, x2, y2]

            conf_threshold: Confidence threshold for detections (0.0-1.0)    confidence: float

            bytetrack_threshold: Confidence threshold for ByteTrack certainty (0.0-1.0)    class_id: int

        """    feature: Optional[np.ndarray] = None  # Re-ID features

        self.model = yolo_model    

        self.device = device    @property

        self.conf_threshold = conf_threshold    def center(self) -> np.ndarray:

        self.bytetrack_threshold = bytetrack_threshold        """Get center point of bounding box"""

                return np.array([(self.bbox[0] + self.bbox[2]) / 2, 

        # Initialize ByteTrack tracker per camera                        (self.bbox[1] + self.bbox[3]) / 2])

        self.byte_trackers = {}    

            @property

        # Initialize DeepSORT tracker per camera (for uncertain tracks)    def area(self) -> float:

        self.deepsort_trackers = {}        """Get area of bounding box"""

                return (self.bbox[2] - self.bbox[0]) * (self.bbox[3] - self.bbox[1])

        # Tracking data for each camera

        self.camera_tracks = defaultdict(lambda: {

            'count': 0,@dataclass

            'tracks': {},class Track:

            'history': deque(maxlen=100),    """Tracked object with Kalman filter and history"""

            'last_update': None,    track_id: int

            'bytetrack_confident': 0,    kalman_filter: KalmanFilter

            'deepsort_assisted': 0,    state: TrackState = TrackState.NEW

            'last_frame': None    hits: int = 1

        })    hit_streak: int = 1

            age: int = 1

        logger.info("âœ… Tracking service initialized")    time_since_update: int = 0

        logger.info("ðŸš€ ByteTrack initialized (30 FPS tracking)")    features: deque = field(default_factory=lambda: deque(maxlen=30))

        logger.info("ðŸ” DeepSORT ReID initialized (fallback for uncertain tracks)")    history: deque = field(default_factory=lambda: deque(maxlen=30))

        confidence_history: deque = field(default_factory=lambda: deque(maxlen=10))

    def _get_bytetrack_tracker(self, camera_id: int) -> sv.ByteTrack:    

        """Get or create ByteTrack tracker for camera"""    def predict(self):

        if camera_id not in self.byte_trackers:        """Predict next state using Kalman filter"""

            self.byte_trackers[camera_id] = sv.ByteTrack(        if self.time_since_update > 0:

                track_activation_threshold=0.5,  # Higher threshold for activation            self.hit_streak = 0

                lost_track_buffer=30,  # Frames to keep lost tracks (1 second at 30 FPS)        self.kalman_filter.predict()

                minimum_matching_threshold=0.8,  # High matching threshold        self.age += 1

                frame_rate=30  # 30 FPS processing        self.time_since_update += 1

            )        return self.kalman_filter.x[:4].flatten()

            logger.info(f"ðŸš€ ByteTrack tracker created for camera {camera_id}")    

        return self.byte_trackers[camera_id]    def update(self, detection: Detection):

            """Update track with new detection"""

    def _get_deepsort_tracker(self, camera_id: int) -> DeepSort:        self.time_since_update = 0

        """Get or create DeepSORT tracker for camera with GPU acceleration"""        self.hits += 1

        if camera_id not in self.deepsort_trackers:        self.hit_streak += 1

            # Use GPU for DeepSORT ReID embeddings if available        

            import torch        # Update Kalman filter

            embedder_gpu = torch.cuda.is_available()        measurement = self._bbox_to_measurement(detection.bbox)

                    self.kalman_filter.update(measurement)

            self.deepsort_trackers[camera_id] = DeepSort(        

                max_age=30,  # Maximum frames to keep lost tracks        # Store history

                n_init=3,  # Minimum consecutive detections for track initialization        self.history.append(detection.bbox.copy())

                nms_max_overlap=0.7,  # Non-max suppression overlap threshold        self.confidence_history.append(detection.confidence)

                max_cosine_distance=0.3,  # Maximum cosine distance for ReID matching        

                nn_budget=100,  # Maximum size of appearance descriptor gallery        # Store features for re-identification

                embedder="mobilenet",  # Use MobileNet for ReID embeddings        if detection.feature is not None:

                embedder_gpu=embedder_gpu,  # Use GPU for embeddings            self.features.append(detection.feature)

                embedder_wts=None,  # Use default weights        

                polygon=False,  # Use bounding boxes, not polygons        # Update state

                today=None        if self.state == TrackState.NEW and self.hits >= 3:

            )            self.state = TrackState.TRACKED

            if embedder_gpu:    

                logger.info(f"ðŸŽ® DeepSORT using GPU for camera {camera_id}")    @property

            else:    def bbox(self) -> np.ndarray:

                logger.info(f"DeepSORT using CPU for camera {camera_id}")        """Get current bounding box from Kalman state"""

        return self.deepsort_trackers[camera_id]        state = self.kalman_filter.x[:4].flatten()

            return self._measurement_to_bbox(state)

    def detect_people(self, frame: np.ndarray, camera_id: int = None) -> Tuple[sv.Detections, List[Dict]]:    

        """    @property

        Detect people in a frame using YOLO on GPU    def avg_confidence(self) -> float:

                """Get average confidence from recent detections"""

        Args:        if not self.confidence_history:

            frame: Input frame (numpy array)            return 0.0

            camera_id: Camera identifier (optional, for tracking stats)        return float(np.mean(self.confidence_history))

                

        Returns:    @staticmethod

            Tuple of (supervision Detections, detections list for compatibility)    def _bbox_to_measurement(bbox: np.ndarray) -> np.ndarray:

        """        """Convert [x1,y1,x2,y2] to [cx,cy,w,h]"""

        # Run YOLO detection on GPU - only detect persons (class 0)        w = bbox[2] - bbox[0]

        results = self.model.predict(        h = bbox[3] - bbox[1]

            frame,        cx = bbox[0] + w / 2

            classes=[0],  # Person class only        cy = bbox[1] + h / 2

            conf=self.conf_threshold,        return np.array([cx, cy, w, h])

            iou=0.7,  # IoU threshold for NMS    

            verbose=False,    @staticmethod

            device=self.device,  # Use GPU    def _measurement_to_bbox(measurement: np.ndarray) -> np.ndarray:

            half=True if self.device == 'cuda' else False  # FP16 for faster inference on GPU        """Convert [cx,cy,w,h] to [x1,y1,x2,y2]"""

        )        cx, cy, w, h = measurement

                return np.array([cx - w/2, cy - h/2, cx + w/2, cy + h/2])

        # Convert to supervision Detections format for ByteTrack

        detections_sv = sv.Detections.from_ultralytics(results[0])

        def iou(bbox1: np.ndarray, bbox2: np.ndarray) -> float:

        # Also create legacy format for compatibility    """Calculate IoU between two bounding boxes"""

        detections_list = []    x1 = max(bbox1[0], bbox2[0])

        if len(results) > 0 and results[0].boxes is not None:    y1 = max(bbox1[1], bbox2[1])

            boxes = results[0].boxes    x2 = min(bbox1[2], bbox2[2])

            for box in boxes:    y2 = min(bbox1[3], bbox2[3])

                # Extract box coordinates    

                x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()    inter_area = max(0, x2 - x1) * max(0, y2 - y1)

                confidence = float(box.conf[0].cpu().numpy())    bbox1_area = (bbox1[2] - bbox1[0]) * (bbox1[3] - bbox1[1])

                    bbox2_area = (bbox2[2] - bbox2[0]) * (bbox2[3] - bbox2[1])

                detection = {    union_area = bbox1_area + bbox2_area - inter_area

                    'bbox': [int(x1), int(y1), int(x2), int(y2)],    

                    'confidence': confidence,    return inter_area / union_area if union_area > 0 else 0.0

                    'center': (int((x1 + x2) / 2), int((y1 + y2) / 2))

                }

                detections_list.append(detection)def cosine_distance(feat1: np.ndarray, feat2: np.ndarray) -> float:

            """Calculate cosine distance between two feature vectors"""

        return detections_sv, detections_list    return 1 - np.dot(feat1, feat2) / (np.linalg.norm(feat1) * np.linalg.norm(feat2) + 1e-5)

    

    def update(self, camera_id: int, frame: np.ndarray) -> Tuple[int, List[Dict], np.ndarray]:

        """class ByteTracker:

        Detect and track people using ByteTrack (30 FPS) with DeepSORT fallback    """

            ByteTrack implementation for high-performance multi-object tracking

        Args:    Handles both high and low confidence detections

            camera_id: Camera identifier    """

            frame: Input frame    

                def __init__(

        Returns:        self,

            Tuple of (unique_person_count, track_info_list, annotated_frame)        track_thresh: float = 0.5,

        """        track_buffer: int = 30,

        # Step 1: Detect people in current frame        match_thresh: float = 0.8,

        detections_sv, detections_list = self.detect_people(frame, camera_id)        min_box_area: float = 10,

                low_thresh: float = 0.1

        # Step 2: Apply ByteTrack tracking (primary tracker)    ):

        byte_tracker = self._get_bytetrack_tracker(camera_id)        self.track_thresh = track_thresh

        tracked_detections = byte_tracker.update_with_detections(detections_sv)        self.track_buffer = track_buffer

                self.match_thresh = match_thresh

        # Step 3: Identify uncertain tracks (low confidence or new tracks)        self.min_box_area = min_box_area

        uncertain_detections = []        self.low_thresh = low_thresh

        confident_tracks = {}        

                self.tracks: List[Track] = []

        for i, track_id in enumerate(tracked_detections.tracker_id):        self.next_id = 1

            confidence = tracked_detections.confidence[i]        self.frame_id = 0

            bbox = tracked_detections.xyxy[i]        

                    logger.info("ByteTracker initialized")

            # Check if ByteTrack is confident about this track    

            if confidence >= self.bytetrack_threshold:    def update(self, detections: List[Detection]) -> List[Track]:

                # ByteTrack is confident - use it directly        """

                confident_tracks[int(track_id)] = {        Update tracks with new detections

                    'bbox': [int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])],        Implements ByteTrack's two-stage association

                    'confidence': float(confidence),        """

                    'center': (int((bbox[0] + bbox[2]) / 2), int((bbox[1] + bbox[3]) / 2)),        self.frame_id += 1

                    'source': 'bytetrack',        

                    'last_seen': datetime.now()        # Separate high and low confidence detections

                }        high_dets = [d for d in detections if d.confidence >= self.track_thresh]

                self.camera_tracks[camera_id]['bytetrack_confident'] += 1        low_dets = [d for d in detections if self.low_thresh <= d.confidence < self.track_thresh]

            else:        

                # ByteTrack is uncertain - prepare for DeepSORT ReID        # Predict all tracks

                uncertain_detections.append({        for track in self.tracks:

                    'bbox': [int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])],            track.predict()

                    'confidence': float(confidence),        

                    'bytetrack_id': int(track_id)        # First association with high confidence detections

                })        unmatched_tracks, unmatched_dets = self._first_association(high_dets)

                

        # Step 4: Apply DeepSORT ReID for uncertain tracks ONLY        # Second association with low confidence detections

        if len(uncertain_detections) > 0:        unmatched_tracks = self._second_association(unmatched_tracks, low_dets)

            deepsort_tracker = self._get_deepsort_tracker(camera_id)        

                    # Handle unmatched tracks

            # Prepare detections for DeepSORT (needs [bbox, confidence, class])        for track_idx in unmatched_tracks:

            deepsort_input = []            track = self.tracks[track_idx]

            for det in uncertain_detections:            if track.time_since_update > self.track_buffer:

                bbox = det['bbox']                track.state = TrackState.REMOVED

                # DeepSORT expects [left, top, width, height]        

                deepsort_bbox = [bbox[0], bbox[1], bbox[2] - bbox[0], bbox[3] - bbox[1]]        # Remove dead tracks

                deepsort_input.append((deepsort_bbox, det['confidence'], 'person'))        self.tracks = [t for t in self.tracks if t.state != TrackState.REMOVED]

                    

            # Update DeepSORT tracker        # Get active tracks

            deepsort_tracks = deepsort_tracker.update_tracks(deepsort_input, frame=frame)        active_tracks = [t for t in self.tracks if t.state == TrackState.TRACKED]

                    

            # Add DeepSORT tracks to results        return active_tracks

            for track in deepsort_tracks:    

                if track.is_confirmed():    def _first_association(self, detections: List[Detection]) -> Tuple[List[int], List[int]]:

                    bbox = track.to_ltrb()  # Get [left, top, right, bottom]        """First association stage with high confidence detections"""

                    track_id = track.track_id        if not detections:

                                return list(range(len(self.tracks))), []

                    # Convert track_id to int and use negative IDs to distinguish DeepSORT from ByteTrack        

                    try:        # Calculate IoU matrix

                        numeric_track_id = int(track_id)        iou_matrix = np.zeros((len(self.tracks), len(detections)))

                        deepsort_key = -numeric_track_id        for t_idx, track in enumerate(self.tracks):

                    except (ValueError, TypeError):            for d_idx, det in enumerate(detections):

                        # If conversion fails, use a hash-based approach                iou_matrix[t_idx, d_idx] = iou(track.bbox, det.bbox)

                        deepsort_key = f"ds_{track_id}"        

                            # Hungarian algorithm for matching

                    confident_tracks[deepsort_key] = {        track_indices, det_indices = linear_sum_assignment(-iou_matrix)

                        'bbox': [int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3])],        

                        'confidence': track.get_det_conf() if hasattr(track, 'get_det_conf') else 0.5,        matches = []

                        'center': (int((bbox[0] + bbox[2]) / 2), int((bbox[1] + bbox[3]) / 2)),        unmatched_tracks = []

                        'source': 'deepsort',        unmatched_dets = list(range(len(detections)))

                        'last_seen': datetime.now()        

                    }        for t_idx, d_idx in zip(track_indices, det_indices):

                    self.camera_tracks[camera_id]['deepsort_assisted'] += 1            if iou_matrix[t_idx, d_idx] < self.match_thresh:

                        unmatched_tracks.append(t_idx)

        # Update camera tracks            else:

        self.camera_tracks[camera_id]['tracks'] = confident_tracks                matches.append((t_idx, d_idx))

        self.camera_tracks[camera_id]['count'] = len(confident_tracks)                if d_idx in unmatched_dets:

        self.camera_tracks[camera_id]['last_update'] = datetime.now()                    unmatched_dets.remove(d_idx)

        self.camera_tracks[camera_id]['last_frame'] = frame        

                # Update matched tracks

        # Prepare track info for response        for t_idx, d_idx in matches:

        track_info = [            self.tracks[t_idx].update(detections[d_idx])

            {        

                'track_id': track_id,        # Create new tracks for unmatched detections

                'bbox': track_data['bbox'],        for d_idx in unmatched_dets:

                'confidence': track_data['confidence'],            self._initiate_track(detections[d_idx])

                'source': track_data['source']        

            }        # Get unmatched track indices

            for track_id, track_data in confident_tracks.items()        matched_track_ids = [m[0] for m in matches]

        ]        unmatched_tracks.extend([i for i in range(len(self.tracks)) if i not in matched_track_ids])

                

        # Draw annotations on frame        return unmatched_tracks, []

        annotated_frame = self.draw_tracks(frame, camera_id)    

            def _second_association(self, unmatched_tracks: List[int], detections: List[Detection]) -> List[int]:

        return len(confident_tracks), track_info, annotated_frame        """Second association stage with low confidence detections"""

            if not detections or not unmatched_tracks:

    def draw_tracks(self, frame: np.ndarray, camera_id: int) -> np.ndarray:            return unmatched_tracks

        """        

        Draw bounding boxes and IDs on frame - minimal clean overlay (BRINKSv2 style)        # Only match with lost tracks

                lost_tracks = [idx for idx in unmatched_tracks 

        Args:                      if self.tracks[idx].state == TrackState.LOST]

            frame: Input frame        

            camera_id: Camera identifier        if not lost_tracks:

                        return unmatched_tracks

        Returns:        

            Annotated frame with tracking visualization        # Calculate IoU matrix

        """        iou_matrix = np.zeros((len(lost_tracks), len(detections)))

        annotated = frame.copy()        for i, t_idx in enumerate(lost_tracks):

                    for d_idx, det in enumerate(detections):

        # Get tracks for this camera                iou_matrix[i, d_idx] = iou(self.tracks[t_idx].bbox, det.bbox)

        tracks = self.camera_tracks[camera_id].get('tracks', {})        

        people_count = self.camera_tracks[camera_id].get('count', 0)        # Hungarian algorithm

                track_indices, det_indices = linear_sum_assignment(-iou_matrix)

        # Draw each track with minimal design        

        for track_id, track_data in tracks.items():        matches = []

            bbox = track_data['bbox']        for i, d_idx in zip(track_indices, det_indices):

            confidence = track_data.get('confidence', 0.0)            if iou_matrix[i, d_idx] >= 0.5:  # Lower threshold for second stage

            source = track_data.get('source', 'unknown')                t_idx = lost_tracks[i]

                            self.tracks[t_idx].update(detections[d_idx])

            # Color coding: Green for ByteTrack, Blue for DeepSORT                matches.append(t_idx)

            if source == 'bytetrack':        

                color = (0, 255, 0)  # Green for ByteTrack        # Return still unmatched tracks

            elif source == 'deepsort':        return [idx for idx in unmatched_tracks if idx not in matches]

                color = (255, 165, 0)  # Orange for DeepSORT    

            else:    def _initiate_track(self, detection: Detection):

                color = (255, 255, 255)  # White for unknown        """Create a new track from detection"""

                    # Initialize Kalman filter

            # Draw bounding box with clean line        kf = KalmanFilter(dim_x=8, dim_z=4)

            cv2.rectangle(annotated, (bbox[0], bbox[1]), (bbox[2], bbox[3]), color, 2)        

                    # State transition matrix

            # Prepare label        kf.F = np.array([[1,0,0,0,1,0,0,0],

            try:                        [0,1,0,0,0,1,0,0],

                if isinstance(track_id, str):                        [0,0,1,0,0,0,1,0],

                    if track_id.startswith("ds_"):                        [0,0,0,1,0,0,0,1],

                        abs_track_id = track_id[3:]                        [0,0,0,0,1,0,0,0],

                    else:                        [0,0,0,0,0,1,0,0],

                        abs_track_id = track_id                        [0,0,0,0,0,0,1,0],

                else:                        [0,0,0,0,0,0,0,1]])

                    abs_track_id = abs(int(track_id))        

            except (ValueError, TypeError):        # Measurement function

                abs_track_id = "?"        kf.H = np.array([[1,0,0,0,0,0,0,0],

                                    [0,1,0,0,0,0,0,0],

            label = f"#{abs_track_id}"                        [0,0,1,0,0,0,0,0],

                                    [0,0,0,1,0,0,0,0]])

            # Draw label above bounding box        

            font = cv2.FONT_HERSHEY_SIMPLEX        # Measurement uncertainty

            font_scale = 0.6        kf.R *= 10.0

            thickness = 2        

            (label_width, label_height), baseline = cv2.getTextSize(label, font, font_scale, thickness)        # Process uncertainty

                    kf.Q[-1,-1] *= 0.01

            # Position label above box        kf.Q[4:,4:] *= 0.01

            label_y = bbox[1] - 8        

            if label_y < label_height + 5:        # Initial state

                label_y = bbox[1] + label_height + 8        measurement = Track._bbox_to_measurement(detection.bbox)

                    kf.x[:4] = measurement.reshape((4, 1))

            # Draw label background (semi-transparent)        kf.x[4:] = 0

            cv2.rectangle(annotated,         

                         (bbox[0], label_y - label_height - 5),        # Create track

                         (bbox[0] + label_width + 10, label_y + 3),        track = Track(

                         color, -1)            track_id=self.next_id,

                        kalman_filter=kf,

            # Draw label text in black for contrast            state=TrackState.NEW

            cv2.putText(annotated, label, (bbox[0] + 5, label_y - 1),        )

                       font, font_scale, (0, 0, 0), thickness)        track.update(detection)

                

        # Add people count at top        self.tracks.append(track)

        count_text = f"People: {people_count}"        self.next_id += 1

        cv2.putText(annotated, count_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)

        

        # Add tracking statsclass TrackingService:

        bytetrack_count = self.camera_tracks[camera_id].get('bytetrack_confident', 0)    """

        deepsort_count = self.camera_tracks[camera_id].get('deepsort_assisted', 0)    Main tracking service with conditional strategy selection

        stats_text = f"BT:{bytetrack_count} DS:{deepsort_count}"    Uses ByteTrack for primary tracking (fast, GPU-accelerated)

        cv2.putText(annotated, stats_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)    DeepSORT only for re-identification when tracks are lost (optional, not implemented yet)

            """

        return annotated    

        def __init__(

    def get_statistics(self) -> Dict:        self,

        """Get tracking performance statistics"""        strategy: str = "bytetrack",  # "bytetrack" or "adaptive"

        total_bytetrack = sum(cam['bytetrack_confident'] for cam in self.camera_tracks.values())        track_thresh: float = 0.5,

        total_deepsort = sum(cam['deepsort_assisted'] for cam in self.camera_tracks.values())        match_thresh: float = 0.8,

                use_gpu: bool = True

        return {    ):

            'total_cameras': len(self.camera_tracks),        self.strategy = strategy

            'bytetrack_trackers': len(self.byte_trackers),        self.use_gpu = use_gpu

            'deepsort_trackers': len(self.deepsort_trackers),        self.device = self._detect_device()

            'total_bytetrack_tracks': total_bytetrack,        

            'total_deepsort_tracks': total_deepsort,        self.tracker = ByteTracker(

            'active_tracks': sum(cam['count'] for cam in self.camera_tracks.values())            track_thresh=track_thresh,

        }            match_thresh=match_thresh

            )

    def reset(self, camera_id: int):        

        """Reset tracker state for a camera"""        # Statistics

        if camera_id in self.byte_trackers:        self.stats = {

            del self.byte_trackers[camera_id]            "total_frames": 0,

        if camera_id in self.deepsort_trackers:            "total_detections": 0,

            del self.deepsort_trackers[camera_id]            "total_tracks": 0,

        if camera_id in self.camera_tracks:            "avg_processing_time": 0.0,

            del self.camera_tracks[camera_id]            "tracks_created": 0,

        logger.info(f"Tracker reset for camera {camera_id}")            "tracks_lost": 0

        }
        
        # Performance monitoring
        self.processing_times = deque(maxlen=100)
        
        logger.info(f"TrackingService initialized with strategy: {strategy}, device: {self.device}")
    
    def _detect_device(self) -> str:
        """Detect GPU availability"""
        if not self.use_gpu:
            logger.info("GPU disabled by configuration - using CPU")
            return "cpu"
        
        try:
            import torch
            if torch.cuda.is_available():
                logger.info(f"ðŸš€ Tracking service using GPU acceleration")
                return "cuda"
            else:
                logger.info("No GPU available for tracking - using CPU")
                return "cpu"
        except ImportError:
            logger.info("PyTorch not available - using CPU for tracking")
            return "cpu"
    
    def update(
        self, 
        frame: np.ndarray, 
        detections_data: List[dict]
    ) -> Tuple[int, List[Dict], np.ndarray]:
        """
        Update tracking with new frame and detections
        Optimized for speed - minimal overhead
        
        Args:
            frame: Current video frame
            detections_data: List of detection dicts from YOLO
        
        Returns:
            Tuple of (unique_person_count, track_info_list, annotated_frame)
        """
        start_time = time.time()
        
        # Convert detection dicts to Detection objects (fast)
        detections = [
            Detection(
                bbox=np.array(d["bbox"], dtype=np.float32),  # Specify dtype for speed
                confidence=d["confidence"],
                class_id=d["class_id"],
                feature=d.get("feature")
            )
            for d in detections_data
        ]
        
        # Update tracker (ByteTrack is fast)
        active_tracks = self.tracker.update(detections)
        
        # Get unique count
        unique_count = len(active_tracks)
        
        # Prepare track info (minimal)
        track_info = [
            {
                "track_id": track.track_id,
                "bbox": track.bbox.tolist(),
                "confidence": track.avg_confidence
            }
            for track in active_tracks
        ]
        
        # Annotate frame (optimized - works in-place)
        annotated_frame = self._annotate_frame(frame, active_tracks)
        
        # Update statistics
        processing_time = time.time() - start_time
        self.processing_times.append(processing_time)
        self._update_stats(len(detections), len(active_tracks))
        
        return unique_count, track_info, annotated_frame
    
    def _annotate_frame(self, frame: np.ndarray, tracks: List[Track]) -> np.ndarray:
        """
        Draw lightweight tracking visualization - optimized for speed
        Only draws essential information to minimize processing time
        """
        # Work directly on frame to avoid copy overhead
        annotated = frame
        
        # Pre-calculate font settings
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.6
        font_thickness = 2
        box_thickness = 3
        
        for track in tracks:
            bbox = track.bbox.astype(int)
            color = (0, 255, 0)  # Bright green
            
            # Draw bounding box (most important)
            cv2.rectangle(annotated, (bbox[0], bbox[1]), (bbox[2], bbox[3]), color, box_thickness)
            
            # Draw track ID label (minimal)
            label = f"ID:{track.track_id}"
            (label_w, label_h), baseline = cv2.getTextSize(label, font, font_scale, font_thickness)
            label_y = max(bbox[1] - 10, label_h + 10)
            
            # Simple background rectangle
            cv2.rectangle(annotated, (bbox[0], label_y - label_h - baseline - 5), 
                         (bbox[0] + label_w + 10, label_y + baseline + 5), color, -1)
            
            # White text
            cv2.putText(annotated, label, (bbox[0] + 5, label_y),
                       font, font_scale, (255, 255, 255), font_thickness)
        
        # Simple stats overlay (minimal)
        fps = 1.0/np.mean(self.processing_times) if len(self.processing_times) > 0 else 0
        stats_text = f"Tracks:{len(tracks)} FPS:{fps:.1f}"
        
        # Black background
        cv2.rectangle(annotated, (5, 5), (220, 35), (0, 0, 0), -1)
        cv2.putText(annotated, stats_text, (10, 25),
                   font, 0.5, (0, 255, 0), 1)
        
        return annotated
    
    @staticmethod
    def _get_track_color(track_id: int) -> Tuple[int, int, int]:
        """Generate consistent color for track ID - using standard green for tracking"""
        # Use bright green as standard tracking color (BGR format)
        return (0, 255, 0)  # Green in BGR
    
    def _update_stats(self, num_detections: int, num_tracks: int):
        """Update tracking statistics"""
        self.stats["total_frames"] += 1
        self.stats["total_detections"] += num_detections
        self.stats["total_tracks"] = num_tracks
        self.stats["avg_processing_time"] = float(np.mean(self.processing_times))
    
    def get_statistics(self) -> Dict:
        """Get tracking performance statistics"""
        return {
            **self.stats,
            "tracks_created": self.tracker.next_id - 1,
            "active_tracks": len(self.tracker.tracks)
        }
    
    def reset(self):
        """Reset tracker state"""
        self.tracker.tracks = []
        self.tracker.next_id = 1
        self.tracker.frame_id = 0
        logger.info("Tracker reset")
